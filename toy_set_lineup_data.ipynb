{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data in training and val set\n",
    "import glob\n",
    "import os.path as osp\n",
    "from shutil import copyfile\n",
    "\n",
    "paths = glob.glob('../training/**/*.jpg',recursive=True)\n",
    "\n",
    "for i in range(0,8612):\n",
    "    path = paths[i]\n",
    "    new_path = osp.join('../training/toyset/train2014',path.split('/')[-1])\n",
    "    copyfile(path,new_path)\n",
    "\n",
    "for i in range(8612,10765):\n",
    "    path = paths[i]\n",
    "    new_path = osp.join('../training/toyset/val2014',path.split('/')[-1])\n",
    "    copyfile(path,new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store all ids\n",
    "all_ids = [path.split('/')[-1].split('.')[0] for path in paths]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Ingredient json and recreate it \n",
    "def extract_image_id(json_filename,all_ids):\n",
    "    with open(json_filename, 'rb') as input_file:\n",
    "        objects = ijson.items(input_file, 'item')\n",
    "        all_ingredients = []a\n",
    "        for obj in objects:\n",
    "#            if obj['id'] in all_ids:\n",
    "#                ingredient_list = [lil_dict['text'] for lil_dict in obj['ingredients']]\n",
    "#                all_ingredients.append([obj['id'],ingredient_list])\n",
    "            all_ingredients.append(obj['id'])\n",
    "    return all_ingredients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "#model = word2vec.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "all_dicts = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "with open('../recipe1M_layers/layer1.json', 'r') as json_file:\n",
    "    for item in ijson.items(json_file, \"item\"):\n",
    "        if item['partition'] == 'train':\n",
    "            if item['id'] in all_ids[0:8612]:\n",
    "                ingredients = item['ingredients'] \n",
    "                ingredients_list = [im['text'] for im in ingredients]\n",
    "                tokenized_title = nltk.word_tokenize(item['title'])\n",
    "                food_words = ingredients_list\n",
    "                for token in tokenized_title:\n",
    "                    food_words.append(token)\n",
    "            \n",
    "                description = ','.join(food_words)\n",
    "                tokens = description.split(',')\n",
    "                sentId = 60000 + i  ## just some random numbers\n",
    "                imgId=i\n",
    "                fileName=item[id]+'.jpg'\n",
    "\n",
    "                mini_dict={\"filepath\":\"train2014\",\n",
    "                           \"sentids\":[sentId],\n",
    "                           \"filename\":fileName,\n",
    "                           \"imgid\":imgId,\n",
    "                           \"split\":\"train\",\n",
    "                           \"sentences\":[{\"tokens\":tokens,\n",
    "                                         \"raw\":description,\n",
    "                                         \"sentid\":sentId,\n",
    "                                         \"imgid\":imgId}]}\n",
    "                i +=1\n",
    "                all_dicts.append(mini_dict)\n",
    "            elif item['id'] in all_ids[8612:]\n",
    "                ingredients = item['ingredients'] \n",
    "                ingredients_list = [im['text'] for im in ingredients]\n",
    "                tokenized_title = nltk.word_tokenize(item['title'])\n",
    "                food_words = ingredients_list\n",
    "                for token in tokenized_title:\n",
    "                    food_words.append(token)\n",
    "            \n",
    "                description = ','.join(food_words)\n",
    "                tokens = description.split(',')\n",
    "                sentId = 60000 + i\n",
    "                imgId=i\n",
    "                fileName=item[id]+'.jpg'\n",
    "\n",
    "                mini_dict={\"filepath\":\"train2014\",\n",
    "                           \"sentids\":[sentId],\n",
    "                           \"filename\":fileName,\n",
    "                           \"imgid\":imgId,\n",
    "                           \"split\":\"val\",\n",
    "                           \"sentences\":[{\"tokens\":tokens,\n",
    "                                         \"raw\":description,\n",
    "                                         \"sentid\":sentId,\n",
    "                                         \"imgid\":imgId}]}\n",
    "                i +=1\n",
    "\n",
    "final_dict = {'images':all_dicts}\n",
    "\n",
    "with open('toydata_ingredients.json', 'w') as outfile:\n",
    "    json.dump(final_dict, outfile)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After generating toydata_ingredients.json,\n",
    "## Run create_input_file.py to generate the tokens vectors \n",
    "## correspondence matrix (hsf5 files) necessary to run image \n",
    "## captioning model, When everything is ready, run train.py\n",
    "\n",
    "## Visualize data in VisualizeCapioning.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
